{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.gSCAN_dataset import GroundedScanDataset\n",
    "from seq2seq.model import Model\n",
    "from seq2seq.rollout import Rollout\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import numpy as np\n",
    "device = torch.device(type='cuda')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = GroundedScanDataset(\"gSCAN_data/data/compositional_splits/dataset.txt\", \"gSCAN_data/data/compositional_splits/\", split=\"train\", \n",
    "                                        input_vocabulary_file=\"training_input_vocab.txt\", \n",
    "                                        target_vocabulary_file=\"training_target_vocab.txt\", \n",
    "                                        generate_vocabulary=False, k=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = GroundedScanDataset(\"gSCAN_data/data/compositional_splits/dataset.txt\", \"gSCAN_data/data/compositional_splits/\", split=\"test\", \n",
    "                                        input_vocabulary_file=\"training_input_vocab.txt\", \n",
    "                                        target_vocabulary_file=\"training_target_vocab.txt\", \n",
    "                                        generate_vocabulary=False, k=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = training_set.dataset.get_examples_with_image(\"train\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, e in enumerate(example):\n",
    "    print(e)\n",
    "    input_commands = e[\"input_command\"]\n",
    "    target_commands = e[\"target_command\"]\n",
    "    situation_image = e[\"situation_image\"]\n",
    "    plt.imshow(situation_image[:,:,3])\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = testing_set.dataset.get_examples_with_image(\"test\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, e in enumerate(test_example):\n",
    "    print(e)\n",
    "    input_commands = e[\"input_command\"]\n",
    "    target_commands = e[\"target_command\"]\n",
    "    situation_image = e[\"situation_image\"]\n",
    "    plt.imshow(situation_image[:,:,3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.read_dataset(max_examples=100,\n",
    "                              simple_situation_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'data_path': 'gSCAN_data/data/compositional_splits/dataset.txt', 'data_directory': 'gSCAN_data/data/compositional_splits/', 'generate_vocabularies': False, 'input_vocab_path': 'training_input_vocab.txt', 'target_vocab_path': 'training_target_vocab.txt', 'embedding_dimension': 25, 'num_encoder_layers': 1, 'encoder_dropout_p': 0.3, 'encoder_bidirectional': True, 'training_batch_size': 50, 'test_batch_size': 1, 'max_decoding_steps': 30, 'num_decoder_layers': 1, 'decoder_dropout_p': 0.3, 'cnn_kernel_size': 7, 'cnn_dropout_p': 0.1, 'cnn_hidden_num_channels': 50, 'simple_situation_representation': True, 'decoder_hidden_size': 100, 'encoder_hidden_size': 100, 'learning_rate': 0.001, 'adam_beta_1': 0.9, 'adam_beta_2': 0.999, 'resume_from_file': '', 'max_training_iterations': 200000, 'output_directory': 'models', 'print_every': 100, 'evaluate_every': 1000, 'conditional_attention': True, 'auxiliary_task': False, 'weight_target_loss': 0.3, 'attention_type': 'bahdanau', 'k': 0, 'max_training_examples': None, 'seed': 42, 'kwargs': {'mode': 'train', 'split': 'test', 'max_testing_examples': None, 'splits': 'test', 'output_file_name': 'predict.json'}, 'device': device, 'lr_decay': 0.9, 'lr_decay_steps': 20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/anaconda3/envs/gsCAN/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# model = Model(input_vocabulary_size=training_set.input_vocabulary_size,\n",
    "#                   target_vocabulary_size=training_set.target_vocabulary_size,\n",
    "#                   num_cnn_channels=training_set.image_channels,\n",
    "#                   input_padding_idx=training_set.input_vocabulary.pad_idx,\n",
    "#                   target_pad_idx=training_set.target_vocabulary.pad_idx,\n",
    "#                   target_eos_idx=training_set.target_vocabulary.eos_idx,\n",
    "#                   embedding_dimension=25, encoder_hidden_size=100, num_encoder_layers=1,\n",
    "#                   encoder_dropout_p=0.3, encoder_bidirectional=True, num_decoder_layers=1,\n",
    "#                   decoder_dropout_p=0.3, decoder_hidden_size=100, cnn_kernel_size=7, cnn_dropout_p=0.1,\n",
    "#                   cnn_hidden_num_channels=50, output_directory=\"models\", conditional_attention=True, auxiliary_task=False,\n",
    "#                   simple_situation_representation=True, attention_type=\"bahdanau\")\n",
    "model = Model(input_vocabulary_size=training_set.input_vocabulary_size,\n",
    "                  target_vocabulary_size=training_set.target_vocabulary_size,\n",
    "                  num_cnn_channels=training_set.image_channels,\n",
    "                  input_padding_idx=training_set.input_vocabulary.pad_idx,\n",
    "                  target_pad_idx=training_set.target_vocabulary.pad_idx,\n",
    "                  target_eos_idx=training_set.target_vocabulary.eos_idx,\n",
    "                  **cfg)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (situation_encoder): ConvolutionalNet(\n",
       "    (conv_1): Conv2d(16, 50, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_2): Conv2d(16, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv_3): Conv2d(16, 50, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (layers): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (visual_attention): Attention(\n",
       "    (key_layer): Linear(in_features=150, out_features=100, bias=False)\n",
       "    (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "  )\n",
       "  (encoder): EncoderRNN(\n",
       "    EncoderRNN\n",
       "     bidirectional=True \n",
       "     num_layers=1\n",
       "     hidden_size=100\n",
       "     dropout=0.3\n",
       "     n_input_symbols=21\n",
       "    \n",
       "    (embedding): Embedding(21, 25, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm): LSTM(25, 100, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (enc_hidden_to_dec_hidden): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (textual_attention): Attention(\n",
       "    (key_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "  )\n",
       "  (attention_decoder): BahdanauAttentionDecoderRNN(\n",
       "    AttentionDecoderRNN\n",
       "     num_layers=1\n",
       "     hidden_size=100\n",
       "     dropout=0.3\n",
       "     num_output_symbols=9\n",
       "    \n",
       "    (queries_to_keys): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (embedding): Embedding(9, 100, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm): LSTM(300, 100, dropout=0.3)\n",
       "    (textual_attention): Attention(\n",
       "      (key_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "      (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "      (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "    )\n",
       "    (visual_attention): Attention(\n",
       "      (key_layer): Linear(in_features=150, out_features=100, bias=False)\n",
       "      (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "      (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "    )\n",
       "    (output_to_hidden): Linear(in_features=400, out_features=100, bias=False)\n",
       "    (hidden_to_output): Linear(in_features=100, out_features=9, bias=False)\n",
       "  )\n",
       "  (loss_criterion): NLLLoss()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = Rollout(model, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chang/multimodal_neural_gsCAN/seq2seq/model.py:308: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  token = F.softmax(output).multinomial(1).squeeze()\n"
     ]
    }
   ],
   "source": [
    "for (input_batch, input_lengths, _, situation_batch, _, target_batch, target_lengths, agent_positions, target_positions) in training_set.get_data_iterator(batch_size=10):\n",
    "    target_scores, target_position_scores = model(commands_input=input_batch, commands_lengths=input_lengths,\n",
    "                                                          situations_input=situation_batch, target_batch=target_batch,\n",
    "                                                          target_lengths=target_lengths)\n",
    "    samples = model.sample(batch_size=10, length=max(target_lengths).astype(int), commands_input=input_batch, commands_lengths=input_lengths, \n",
    "                                    situations_input=situation_batch, target_batch=target_batch, sos_idx=training_set.input_vocabulary.sos_idx, eos_idx=training_set.input_vocabulary.eos_idx)\n",
    "    reward = rollout.get_reward(target_scores, 16, input_batch, input_lengths, situation_batch, target_batch, training_set.input_vocabulary.sos_idx, training_set.input_vocabulary.eos_idx, reward_func)\n",
    "    prob = model.pred(commands_input=input_batch, commands_lengths=input_lengths,\n",
    "                                                  situations_input=situation_batch, target_batch=samples,\n",
    "                                                  target_lengths=target_lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "samples = model.sample(F.log_softmax(target_scores, dim=-1).max(dim=-1)[1].detach()[:,:6], \n",
    "             commands_input=input_batch,\n",
    "             commands_lengths=input_lengths,\n",
    "             situations_input=situation_batch, \n",
    "             target_batch=target_batch,\n",
    "             sos_idx=testing_set.target_vocabulary.sos_idx, eos_idx=testing_set.target_vocabulary.eos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 11, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'szie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-de9c95b70533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mszie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'szie'"
     ]
    }
   ],
   "source": [
    "F.log_softmax(target_scores, dim=-1).max(dim=-1)[1].detach().chunk(11, dim=1)[1:].szie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 4, 8, 8, 8, 8, 5, 8, 2, 0, 8, 8, 2, 8, 2, 1, 0, 8, 3, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 2, 8, 8, 8, 8, 8, 3, 5, 3, 4, 2, 8, 4, 8, 2, 8, 4, 8, 2, 6,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 5, 8, 8, 2, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 8, 3, 8, 8, 8, 2, 2, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 4, 5, 8, 8, 8, 8, 3, 8, 8, 3, 4, 5, 3, 2, 6, 2, 8, 1, 0, 8,\n",
       "        2, 1, 0, 8, 2, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 5, 8, 3, 8, 8, 8, 3, 5, 3, 4, 8, 2, 8, 2, 8, 2, 1, 0, 8, 2,\n",
       "        6, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 4, 2, 8, 8, 3, 8, 4, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 5, 8, 8, 2, 8, 2, 8, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 5, 8, 4, 8, 8, 5, 8, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 4, 5, 8, 8, 8, 8, 3, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(model.sample(F.log_softmax(target_scores, dim=-1).max(dim=-1)[1].detach()[:,:6], \n",
    "             commands_input=input_batch,\n",
    "             commands_lengths=input_lengths,\n",
    "             situations_input=situation_batch, \n",
    "             target_batch=target_batch,\n",
    "             sos_idx=testing_set.target_vocabulary.sos_idx, eos_idx=testing_set.target_vocabulary.eos_idx)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-615fb4ee4925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gsCAN/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "F.log_softmax(target_scores, dim=-1).max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(pred):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout.get_reward(target_scores, 16, input_batch, input_lengths, situation_batch, target_batch, testing_set.target_vocabulary.sos_idx, testing_set.target_vocabulary.eos_idx, reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 4, 5, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 2, 0, 0, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 2, 0, 0, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 2, 0, 0, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 4, 5, 2, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 4, 5, 2, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 4, 5, 2, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 4, 5, 2, 0, 0],\n",
       "        [1, 3, 4, 5, 4, 5, 4, 5, 4, 5, 2],\n",
       "        [1, 3, 4, 5, 4, 5, 4, 5, 4, 5, 2]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsCAN",
   "language": "python",
   "name": "gscan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
